---
title: "report_lab3"
author: "Yanjie Lyu, Yi Yang, Qingxuan Cui"
date: "2024-12-11"
output:
  bookdown::pdf_document2:
    number_sections: true
    latex_engine: xelatex
header-includes:
  - \usepackage{float}
editor_options: 
  markdown: 
    wrap: 72
---

# Theory

## In neural networks, what do we mean by mini-batch and epoch?

mini-batch: mini-batch is the subset of training data randomly sampled,
aiming to solve the problem of excessive computation time and memory
space consumption caused by using the entire training set to train the
model and updating the parameters, especially when many of them are
probably relatively similar data points in the training set. A
mini-batch can typically contain 1, 10 or 100 data points, but it is
recommended to set it to the nth power of 2, so that it can be aligned
with the memory size of the GPU. epoch: One complete pass through the
training data is called an epoch. An epoch consists of $\frac{n}{nb}$
where $n$ represents the size of train data, and $nb$ represents the
batch size. (P124-P125)

# Assignment 3 Support Vector Machine

```{r include=FALSE}
# Lab 3 block 1 of 732A99/TDDE01/732A68 Machine Learning
# Author: jose.m.pena@liu.se
# Made for teaching purposes

library(kernlab)
set.seed(1234567890)

data(spam)
foo <- sample(nrow(spam))
spam <- spam[foo,]
tr <- spam[1:3000, ]
va <- spam[3001:3800, ]
trva <- spam[1:3800, ]
te <- spam[3801:4601, ] 

by <- 0.3
err_va <- NULL
for(i in seq(by,5,by)){
  filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=i,scaled=FALSE)
  mailtype <- predict(filter,va[,-58])
  t <- table(mailtype,va[,58])
  err_va <-c(err_va,(t[1,2]+t[2,1])/sum(t))
}
# filter
# trained by train dataset
# find the optimal C on validation dataset


filter0 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter0,va[,-58])
t <- table(mailtype,va[,58])
err0 <- (t[1,2]+t[2,1])/sum(t)
err0
# filter0
# trained by train data
# evaluated by validation data

filter1 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter1,te[,-58])
t <- table(mailtype,te[,58])
err1 <- (t[1,2]+t[2,1])/sum(t)
err1
# filter1
# trained by train data
# evaluated by test data

filter2 <- ksvm(type~.,data=trva,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter2,te[,-58])
t <- table(mailtype,te[,58])
err2 <- (t[1,2]+t[2,1])/sum(t)
err2
# filter2
# trained by train valid data? what's that for?
# evaluated by test data

filter3 <- ksvm(type~.,data=spam,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter3,te[,-58])
t <- table(mailtype,te[,58])
err3 <- (t[1,2]+t[2,1])/sum(t)
err3
# filter3
# trained by whole data
# evaluated by test data

# I chose filter2 and its error 'cause the lower misclassification error in test data.
# But I'm not sure with that.
```

```{r echo=FALSE}
error_rate = c(err0, err1, err2, err3)
error_rate = round(error_rate, 3)
table = data.frame(filter = c("filter0", "filter1", "filter2", "filter3"),
                   error_rate = error_rate)

library(knitr)
kable(t(table), format="markdown")
```

## Which filter do we return to the user ? Why?

Filter2 should be returned to the user.

***Reasons:***

Filter2 was trained by the union of validation data and train data,
evaluated by test data, with the risk that the model overfitting the
distribution of hyperparameter tuning. However, filter2 has the lowest
error rate of any model evaluated using a dataset that did not appear in
training at all.

As for the others: Filter0 was evaluated by validation data, while
validation data have been used in optimization. The performance of the
model may be overestimated. It may “memorize” the distribution of valid
data, but perform poorly on real independent test data.

Filter1 was trained by train data and evaluated by test data. It is the
standard pipeline, but the error rate is higher than filter2.

Filter3 was trained by the whole data set, which contains the test data
set. After that, the model have adapted the test data in some degree.
Thus, it would affects the accuracy of model evaluation.

Answer 2:

Filter3 should be returned to the user.

***Reasons:***

Filter3 is trained using the entire dataset (`spam`), which includes the
training set, validation set, and test set. By leveraging all available data,Filter3
is able to utilize the maximum amount of information to build the most robust model possible.

Once the testing phase is complete and the model has been evaluated, 
the focus shifts to creating the best model for practical use. 
Using all available data to train this model is the optimal choice.

## What is the estimate of the generalization error of the filter returned to the user? Why?

Error2 should be returned to the user. We used the filter2, because it
perform better on new dataset among the models who were evaluated on new
dataset. Discussion required there.

Answer 2:

This ensures that the evaluation is unbiased and reflects the model's ability to generalize to unseen data. Filter2's error is a more accurate estimate of the generalization error because the test set remains independent and has not been contaminated by the training process.

## Implementation of SVM predictions.

```{r include=FALSE}
# 3. Implementation of SVM predictions.
# the size of features is 57
pred_vec = c()
sigma = 0.05
support_vector_indices = alphaindex(filter3)[[1]]
support_vector = spam[support_vector_indices, -58]
sv_labels = spam[support_vector_indices, 58]
sv_labels = ifelse(sv_labels == "spam", -1, 1)

coef = coef(filter3)[[1]]
intercept = - b(filter3)
x = spam[, -58]
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
  k2 = 0
  for(j in 1:length(support_vector_indices)){
    rbf = exp(-sum((x[i,] - support_vector[j,])^2)/(2 * sigma^2))
    k2 = k2 + rbf * coef[j]
  }
  pred_vec=c(pred_vec, k2 + intercept)
}
pred_vec_label = ifelse(pred_vec < 0, -1, 1)


pred_func = as.vector(predict(filter3,spam[1:10,-58], type = "decision"))
pred_func_label = ifelse(pred_func < 0, -1, 1)
```

Based on the table below, we can see the values and labels from manual
prediction and function based prediction. Except the first data point,
the labels are same.

```{r, eval=FALSE}
table_pred = data.frame(pred_vec_label, pred_func_label)
colnames(table_pred) = c("manual prediction label", "prediction label")
row.names(table_pred) = paste0("data point", as.character(1:10))
kable(t(table_pred), format="markdown")

```

# Assignment 4

## Task 1

```{r ,include=FALSE, include=FALSE}
library(neuralnet)
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
# Random initialization of the weights in the interval [-1, 1]
n_input <- 1        
n_hidden <- 10      
n_output <- 1      

weights_input_to_hidden <- n_input * n_hidden
weights_hidden_to_output <- n_hidden * n_output
n_weights <- weights_input_to_hidden + weights_hidden_to_output
bias_hidden <- n_hidden  
bias_output <- n_output
n_weights <- n_weights + bias_hidden + bias_output
winit <- runif(n_weights, min = -1, max = 1)

nn <- neuralnet(Sin ~ Var, data = tr, hidden = 10, act.fct = "logistic", linear.output = TRUE,
                startweights = list(
                  matrix(winit[1:weights_input_to_hidden], nrow = n_input, ncol = n_hidden),
                  matrix(winit[(weights_input_to_hidden + 1):(weights_input_to_hidden + weights_hidden_to_output)],
                         nrow = n_hidden, ncol = n_output)
                ))
# Plot of the training data (black), test data (blue), and predictions (red)

# Comment your results
```
```{r echo=FALSE}
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn,te), col="red", cex=1)
```

The model predictions are very good, except for a small deviation from
var=5 to var=7, which is comparable to the sin values of the test set.

## Task 2

```{r echo=FALSE}
# task 2
activation_functions <- list(
      linear = function(x) x,
      relu = function(x) ifelse(x > 0, x, 0),
      softplus = function(x) log(1 + exp(x))
    )
## linear
nn_lin <- neuralnet(Sin ~ Var, data = tr, hidden = 10, act.fct = activation_functions[["linear"]],
               linear.output = TRUE,startweights = list(
                 matrix(winit[1:weights_input_to_hidden], nrow = n_input, ncol = n_hidden),
                 matrix(winit[(weights_input_to_hidden + 1):(weights_input_to_hidden + weights_hidden_to_output)],
                        nrow = n_hidden, ncol = n_output)
               ))

plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn_lin,te), col="red", cex=1)

## relu

nn_relu <- neuralnet(Sin ~ Var, data = tr, hidden = 10, act.fct = activation_functions[["relu"]],
                    linear.output = TRUE,startweights = list(
                      matrix(winit[1:weights_input_to_hidden], nrow = n_input, ncol = n_hidden),
                      matrix(winit[(weights_input_to_hidden + 1):(weights_input_to_hidden + weights_hidden_to_output)],
                             nrow = n_hidden, ncol = n_output)
                    ))
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn_relu,te), col="red", cex=1)
## softplus

nn_softplus <- neuralnet(Sin ~ Var, data = tr, hidden = 10, act.fct = activation_functions[["softplus"]],
                     linear.output = TRUE,startweights = list(
                       matrix(winit[1:weights_input_to_hidden], nrow = n_input, ncol = n_hidden),
                       matrix(winit[(weights_input_to_hidden + 1):(weights_input_to_hidden + weights_hidden_to_output)],
                              nrow = n_hidden, ncol = n_output)
                     ))

plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn_softplus,te), col="red", cex=1)

## comment your results
```

***Linear function:*** The results of the linear function show that it
cannot be used as an activation function at all, and the output of the
neural network is not properly and appropriately expressed, resulting in
a final output that is far from the labels of the test set and a
distribution that does not match at all. ***Relu function:*** The Relu
function performs well up to var=4, and after var=4, like the linear
function, it is completely unusable as an activation function, i.e., it
does not fit at all to the test set labeling distribution. ***Softplus
function:*** The softplus function basically matches the curve of the
test set labeling.

## Task 3

```{r echo=FALSE}
new_Var <- runif(500, 0, 50)
new_data <- data.frame(Var = new_Var, Sin = sin(new_Var))

new_predictions <- predict(nn, new_data)

plot(new_data$Var, new_data$Sin, col = "blue", main = "Neural Network Predictions on [0, 50]",
     xlab = "x", ylab = "sin(x)", cex = 1, pch = 16,ylim=c(-4,1.5))
points(new_data$Var, predict(nn, new_data), col = "red", cex = 1, pch = 16)
legend("topright", legend = c("True Values", "Predictions"),
       col = c("blue", "red"), pch = 16)

```

Apparently, the model cannot predict based on the data points which are
out of the scale [0,10], while it has good performance on the data
points who fall in this interval although they have not appeared before.

## Task 4: In question (3), the predictions seem to converge to some value. Explain why this happens.

```{r echo=FALSE}
weights = nn$weights
weights = weights[[1]]

hidden_weights = weights[[1]][1,]
output_weights = weights[[1]][2,]
hidden_bias = weights[[2]][1:10]
output_bias = weights[[2]][length(weights[[2]])]


```
