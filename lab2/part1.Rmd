---
title: "part1"
author: "Cui Qingxuan"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: true
    latex_engine: xelatex
---

# Assignment 1

```{r include=FALSE}
data = read.csv("data/tecator.csv")
head(data)
```

```{r include=FALSE}
dataProcessing = function(data, colnames){
    for(i in colnames)
        data[[i]] = NULL
    return(data)
}

dataSplit = function(data, proportion){
    set.seed(12345)
    n = dim(data)[1]
    id = sample(1:n,floor(n*0.5))
    train_data = data[id,]
    test_data = data[-id,]
    data_list = list()
    data_list$train = train_data
    data_list$test = test_data
    return(data_list)
}

computeMSE = function(model ,data, lambda = NULL){
    if(!is.null(lambda)){
      x = as.matrix(data[, -which(names(data) == "Fat")]) 
      y = data$Fat 
      pred = predict(model, newx = x, s = lambda)
      mse = mean((y - pred) ** 2)
    }
    else{
      pred = predict(model, data)
      mse = mean((data$Fat - pred) ** 2)
      return(mse)
    }
}
```

## Quality of fit, prediction and model with linear regression.

```{r include=FALSE}
data = dataProcessing(data, c("Sample", "Protein", "Moisture"))
data_list = dataSplit(data, proportion = 0.5)
train_data = data_list$train
test_data = data_list$test

lr = lm(Fat ~ ., data = train_data)
trainMSE = computeMSE(lr, train_data)
testMSE = computeMSE(lr, test_data)
```

```{r echo=FALSE}
cat("The MSE of training data is:",trainMSE,"\n")
cat("The MSE of test data is:",testMSE,"\n")
```

The model performs well on the training data (MSE = 0.0057), but has a large error on the test data (MSE = 722.4294), indicating that the model cannot be generalized and there is a serious overfitting phenomenon.

## Report the cost function

```{r include=FALSE}
library(glmnet)
x_train = as.matrix(train_data[, -which(names(train_data) == "Fat")]) 
y_train = train_data$Fat  
lasso_model = glmnet(x_train, y_train, alpha = 1)

# Plot the coefficient paths
png("fig/p1_img1.png", width = 1800, height = 1200, res = 300)
plot(lasso_model, xvar = "lambda", label = TRUE)
axis(1, at = log(lasso_model$lambda), labels = floor(lasso_model$lambda)) 
```

The cost function for **LASSO regression** is defined as:

$J(\beta) = \frac{1}{2n} \sum_{i=1}^n \big(y_i - \mathbf{X}_i \cdot \beta \big)^2 + \lambda \sum_{j=1}^p |\beta_j|$

Where lambda controls the regularization strength.

## interpret LASSO Coefficient Paths Plot

```{r echo=FALSE}
cat("\nCoefficients for different lambda values:\n")
plot(lasso_model, xvar = "lambda", label = TRUE)
title(main = "LASSO Coefficient Paths")
```

1.  This graph shows the path that the regression coefficient of each feature in the **LASSO regression model** changes with the regularization parameter $\log(\lambda)$ : the horizontal axis is $\log(\lambda)$, and the vertical axis is the value of the regression coefficient for each feature.

2.  With the increase of $\lambda$ (from right to left), the regularization force is enhanced, and the coefficients of most features are compressed to 0, leaving only a few important features with non-zero coefficients, which realizes feature selection.

## model with only three features

```{r echo=FALSE}
# 确定仅保留3个非零特征的 lambda 值
for (i in seq_along(lasso_model$lambda)) {
  coefs <- coef(lasso_model, s = lasso_model$lambda[i])
  non_zero_count <- sum(coefs != 0) - 1  
  if (non_zero_count == 3) {
    lambda_selected <- lasso_model$lambda[i]
    cat("Lambda with 3 non-zero coefficients:", lambda_selected, "\n")
    break
  }
}

```

```{r echo=FALSE}
coefs <- coef(lasso_model, s = lambda_selected)
coefs_selected_list <- which(coefs!=0)
coefs_selected_list <- coefs_selected_list[-1] # 去掉截距
coefs_selected <- coefs[coefs_selected_list,]
cat("The remaining features are:", "\n")
print(coefs_selected)

```

## fit Ridge regression and compare the plots from steps 3 and 4

```{r echo=FALSE}
ridge_model = glmnet(x_train, y_train, alpha = 0)

plot(ridge_model, xvar = "lambda", label = TRUE)
axis(1, at = log(ridge_model$lambda), labels = floor(ridge_model$lambda)) 


```

LASSO coefficient path diagram:

As $\lambda$ increases, the coefficients of many features are compressed to 0, and the model becomes sparse.

Ridge coefficient path map:

As $\lambda$ increases, the coefficients of all features gradually shrink, but they do not become zero.\

```{r include=FALSE}
cv_lasso = cv.glmnet(x_train, y_train, alpha = 1)
cv_scores = cv_lasso$cvm
# Load necessary libraries
library(ggplot2)

# Extract data for plotting
plot_data = data.frame(
  log_lambda = log(cv_lasso$lambda),  # Log of lambda values
  cv_scores = cv_lasso$cvm           # CV scores (MSE)
)

# Create ggplot
p3 = ggplot(plot_data, aes(x = log_lambda, y = cv_scores)) +
    geom_line(color = "black") +         # Line connecting the points
    geom_point(size = 2, color = "black") +  # Points for CV scores
    geom_vline(xintercept = log(cv_lasso$lambda.min), color = "red", linetype = "dashed", size = 1) +  # Optimal lambda line
    geom_vline(xintercept = log(cv_lasso$lambda.1se), color = "blue", linetype = "dashed", size = 1) + # 1 SE lambda line
    labs(
      title = "Dependence of CV Score on log(λ)",
      x = "log(λ)",
      y = "CV Score (Mean Squared Error)"
    ) +
    annotate("text", x = log(cv_lasso$lambda.min), y = min(cv_lasso$cvm), 
             label = "λ.min", color = "red", hjust = -0.2, vjust = -0.5) +
    annotate("text", x = log(cv_lasso$lambda.1se), y = min(cv_lasso$cvm), 
             label = "λ.1se", color = "blue", hjust = -0.2, vjust = -0.5) +
    theme_minimal()
ggsave("fig/p1_pic3.png", plot = p3, width = 6, height = 4, dpi = 300)

optimal_lambda = cv_lasso$lambda.min
cat("the optimal lambda: ", optimal_lambda,"\n")
coef = coef(cv_lasso, s = optimal_lambda)
var_num = sum(coef != 0) - 1
cat("the number of variables: ", var_num,"\n")

mse_optimal_lam = computeMSE(cv_lasso, test_data, lambda = optimal_lambda)
logminus4_lambda = cv_lasso$lambda[which.min(abs(log(cv_lasso$lambda) + 4))]
mse_certain_lam = computeMSE(cv_lasso, test_data, lambda = logminus4_lambda)
cat("lambda = -4, mse = ", mse_certain_lam, "\n")
cat("lambda = optimal lambda, mse = ", mse_optimal_lam, "\n")
```

## Task 5

### dependence of the CV score on $\log(\lambda)$

```{r echo=FALSE}
p3
```

(comment)

### Optimal $lambda$

```{r echo=FALSE}
cat("the optimal lambda: ", optimal_lambda,"\n")
coef = coef(cv_lasso, s = optimal_lambda)
var_num = sum(coef != 0) - 1
cat("the number of variables: ", var_num,"\n")

mse_optimal_lam = computeMSE(cv_lasso, test_data, lambda = optimal_lambda)
logminus4_lambda = cv_lasso$lambda[which.min(abs(log(cv_lasso$lambda) + 4))]
mse_certain_lam = computeMSE(cv_lasso, test_data, lambda = logminus4_lambda)
cat("log_lambda = -4, mse = ", mse_certain_lam, "\n")
cat("lambda = ",optimal_lambda, "mse = ", mse_optimal_lam, "\n")
```

The difference in MSE values is very small, approximately 0.19, which is unlikely to be statistically significant.

### Scatter plot of the original test versus predicted test values

```{r include=FALSE}
y_test = test_data$Fat
x_test = test_data[, -which(names(test_data) == "Fat")]
y_pred = predict(cv_lasso, s = optimal_lambda, newx = as.matrix(x_test))

p4 = ggplot(data = cbind(y_test, y_pred), mapping = aes(x = y_test, y = y_pred)) +
  geom_point(size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", linewidth = 1) +
  labs(title = "Labels in Test vs Predicted Labels",
       x = "Labels in Test",
       y = "Predicted Labels") +
  theme_minimal()

```

```{r echo=FALSE}
p4
```

The model performs well overall, as most predictions are close to the actual values.

The scatterplot shows that the model captures the general trend of the data, with no significant systematic errors.

But a few outliers at higher values suggest the model may struggle with extreme cases or higher variability in predictions for larger labels.

# Assignment 3

```{r include=FALSE}
communities <- read.csv("data/communities.csv")

#task 1
#scale data except of `ViolentCrimesPerPop`
scaled <- scale(communities[,-which(names(communities)=="ViolentCrimesPerPop")],center=TRUE,scale=TRUE)

#compute eigenvectors and eigenvalues
covariance<- cov(scaled)

#extract eigenvectors and eigenvalues
eigen <- eigen(covariance)
eigen_values <- eigen$values
eigen_vectors <- eigen$vectors
eigen_values
eigen_vectors

#compute total variance
variance <- sum(eigen_values)

#compute contribution of each component
propotion_variance <- eigen_values/variance
propotion_variance

#compute cumulative variance proportion
cumulative_var <- cumsum(propotion_variance)
cumulative_var

#numbers of components to obtain 95% variance
index <-min(which(cumulative_var>0.95))
cat("At least ",index,"components to obtain a 95% of variance in the data.")

#the proportion of variance explained by each of the first two principal component.
first_com_proportion <- propotion_variance[1]
second_com_proportion <- propotion_variance[2]
cat("The proportion of variance of explained by each of the two principal components:\n")
cat("The first components: ",first_com_proportion ,"\n")
cat("The second components: ",second_com_proportion,"\n")




#task 2
library(ggplot2)

#using princomp
prin <- princomp(scaled)

#extract the first principle component
first_com_prin <- prin$scores[,1]

#make the trace plot of the first principle component
df_first <- data.frame(index=1:length(first_com_prin),
                       first_principle_component=first_com_prin)
ggplot(df_first,aes(x=index,y=first_principle_component))+
        geom_line()+
        labs(title = "The trace of the first principle component",x="Index",y="The First Principle Component")+
        theme_minimal()

#Do many features have a notable contribution to this component

#Top 5 features contribute mostly
first_com_contribute <- prin$loadings[,1]
first_com_contribute_df <- data.frame(
  Feature=names(first_com_contribute),
  contribution=first_com_contribute,
  contribution_Abs=abs(first_com_contribute)
)
#extract top 5 features 
top_5_features <- first_com_contribute_df$Feature[order(first_com_contribute_df$contribution_Abs,decreasing = TRUE)[1:5]]
cat("Top 5 features that contributed mostly to the first principle component: ",top_5_features,".\n")


#Relationship to crime level

#medFamInc:median family income (differs from household income for non-family households
#medIncome:median household income 
#PctKids2Par: percentage of kids in family housing with two parents
#pctWInvInc:percentage of households with investment / rent income in 1989
#PctPopUnderPov:percentage of people under the poverty level

# Conclusion: These five features mainly describes the following aspects: 1.Median family income;2.Median household income;3.Percentage of kids in family housing with two parents;4.Percentage of households with investment / rent income;5.Percentage of people under the poverty level.
#All five features are related to family property and household wealth levels.The logical relationship to  crime levels is that  families with fewer financial resources or  more children(which increases financial budget)may be more likely to resort to illegal activities to obtain money.Financial stress within household may be a significant factor contributing to crime level. 



#Plot the pc scores in the coordinates(PC1,PC2)
second_com_prin <- prin$scores[,2]
fir_sec_prin_df <- data.frame(
  PC1=first_com_prin,
  PC2=second_com_prin,
  ViolentCrimesPerPop=communities$ViolentCrimesPerPop
)
ggplot(fir_sec_prin_df,aes(x=PC1,y=PC2,color=ViolentCrimesPerPop))+
  geom_point()+
  scale_color_viridis_c()+
  labs(tittle="PC scores in the coordinates(PC1,PC2)",X="PC1",y="PC2",color="ViolentCrimesPerPop")+
  theme_minimal()
  
#task3
#split the data(50/50)
library(caret)
set.seed(12345)
n <- nrow(communities)
id <- sample(1:n,floor(n*0.5))
train <- communities[id,]
test <- communities[-id,]

scaler <- preProcess(train)
trainS <- predict(scaler,train)
testS <- predict(scaler,test)

#fit the model using scaled training data
fit <- lm(ViolentCrimesPerPop~.-1,data=trainS)

predictions_train <- predict(fit,newdata=trainS)
predictions_test <- predict(fit,newdata=testS)

#MSE for training and test data
MSE_training <- mean((trainS$ViolentCrimesPerPop-predictions_train)^2)
MSE_test <- mean((testS$ViolentCrimesPerPop-predictions_test)^2)
cat("MSE for the training data is :",MSE_training,"\n")
cat("MSE for the test data is :",MSE_test,"\n")

#Comment on the quality of model
R2 <- summary(fit)$r.squared
cat("The R^2 value for the model is: ",R2,"\n")

#Comment:The Mean Squared Error (MSE) for the training data is 0.2752071, indicating that the model performs well on the training set. However, the MSE for the test data is 0.4248011, which is higher than the training data MSE. This suggests that the model may be overfitting, as it performs worse and may not generalize as well on unseen data . The R² value is 0.7245166, meaning that the model explains approximately 72.4% of the variance in the target variable. 

#task4
#implement cost function without intercept
costfunction <- function(theta,training_X,training_y,test_X,test_y){
  #compute the mean squared error
  train_error<- mean((training_X%*%theta-training_y)^2)
  test_error <- mean((test_X%*%theta-test_y)^2)
  return(c(train_error,test_error))
}

training_X <- as.matrix(trainS[,-101])
training_y <- trainS$ViolentCrimesPerPop

test_X <- as.matrix(testS[,-101])
test_y <- testS$ViolentCrimesPerPop

#optimize theta and compute mse
theta_initial <- rep(0,ncol(training_X))


#create a df to store errors each iteration
errors_iteration_df <- data.frame(
  iteration=integer(),
  train_errors=numeric(),
  test_errors=numeric()
)

optim_theta <- optim(par=theta_initial,
                     fn=function(theta){
                       #compute training and test errors for each iteration
                       errors <- costfunction(theta,training_X,training_y,test_X,test_y)
                       
                       #store the errors 
                       errors_iteration_df <<- rbind(errors_iteration_df,data.frame(
                         iteration=nrow(errors_iteration_df)+1,
                         train_errors=errors[1],
                         test_errors=errors[2]
                       ))
                       
                       #return training errors since `optim()` uses training error 
                       return(errors[1])
                       
                     },
                    
                     method = "BFGS")
#print(errors_itteration_df)

#plot showing the dependence if both errors on the iteration number

ggplot(errors_iteration_df[errors_iteration_df$iteration >500,],aes(x=iteration))+   #discard the first 500 iterations to make it visible
  geom_line(aes(y=train_errors,color="Training Errors"))+
  geom_line(aes(y=test_errors,color="Test Errors"))+
  labs(title = "Training and test error during aech iteration",x="Iteration number",y="Errors")+
  scale_color_manual(values = c("blue","red"))+
  scale_y_continuous(limits = c(0, 1.2), breaks = seq(0, 1.2, by = 0.2)) +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5))

print(errors_iteration_df$train_errors)

#find optimal iteration
index <- which.min(errors_iteration_df[,3])
cat("The optimal iteration number is:",index,"\n")

#extract training and test error at optimal iteration
optimail_train_errors <- errors_iteration_df[index,2]
optimai_test_errors <- errors_iteration_df[index,3]


cat("Training error at optimal iteration",optimail_train_errors,"\n")
cat("Test error at optimal iteration",optimai_test_errors,"\n")

```

## Task 1

```{r echo=FALSE}
cat("At least ",index,"components to obtain a 95% of variance in the data.")
```

```{r echo=FALSE}
cat("The proportion of variance of explained by each of the two principal components:\n")
cat("The first components: ",first_com_proportion ,"\n")
cat("The second components: ",second_com_proportion,"\n")
```

## Task 2

### Trace plot of the first principle component.

### Top 5 contributing features

```{r echo=FALSE}
cat("Top 5 features that contributed mostly to the first principle component: ",top_5_features,".\n")
```

medFamInc:median family income (differs from household income for non-family households

medIncome:median household income

PctKids2Par: percentage of kids in family housing with two parents

pctWInvInc:percentage of households with investment / rent income in 1989

PctPopUnderPov:percentage of people under the poverty level

Conclusion:

These five features mainly describes the following aspects:

1.Median family income; 2.Median household income; 3.Percentage of kids in family housing with two parents; 4.Percentage of households with investment / rent income; 5.Percentage of people under the poverty level.

All five features are related to family property and household wealth levels. The logical relationship to crime levels is that families with fewer financial resources or more children (which increases financial budget) may be more likely to resort to illegal activities to obtain money. Financial stress within household may be a significant factor contributing to crime level.

### plot of the PC scores in the coordinates (PC1, PC2)

```{r echo=FALSE}
ggplot(fir_sec_prin_df,aes(x=PC1,y=PC2,color=ViolentCrimesPerPop))+
  geom_point()+
  scale_color_viridis_c()+
  labs(tittle="PC scores in the coordinates(PC1,PC2)",X="PC1",y="PC2",color="ViolentCrimesPerPop")+
  theme_minimal()
```

1.  The score of the data point on the first principal component (PC1) is significantly positively correlated with the violent crime rate ("ViolentCrimesPerPop"), which tends to increase when PC1 increases (yellow).

2.  The second principal component (PC2) has a weak ability to distinguish the violent crime rate, and the distribution of data points in this direction is relatively symmetrical, and there is no obvious pattern.

3.  The overall arc distribution of the data indicates that there may be a nonlinear relationship, and further analysis of the characteristic contribution of PC1 is needed to understand the key factors affecting the violent crime rate.

## Task 3

After training the linear regression model.

```{r echo=FALSE}
cat("MSE for the training data is :",MSE_training,"\n")
cat("MSE for the test data is :",MSE_test,"\n")
cat("The R^2 value for the model is: ",R2,"\n")
```

1.  The MSE of the model on the training data is 0.275, and the MSE on the test data is 0.425. The test error is slightly higher but generally close, indicating that the model is stable and the degree of overfitting is low.

2.  The coefficient of determination R² is 0.725, indicating that the model can explain 72.5% of the variance of violent crime rate, but 27.5% of the variation is still unexplained.

3.  A slight increase in the test error may indicate the need to optimize features or introduce regularization methods to further improve the generalization ability of the model.

## Task 4

# Assignment 4. Theory

## What are the practical approaches for reducing the expected new data error, according to the book?

To achieve minimal $E_{new}$, according to the decomposition $E_{new}$ = $E_{train}$ $+$ generalization gap, we need to have $E_{train}$ as well as the generalization gap small.

1. Increasing the size of the training data to reduce the generalization gap and $E_{new}$;

2. If $E_{hold-out}$ $\approx$ $E_{train}$ (small generalization gap; possibly underfitting), it might be beneficial to increase the model flexibility by loosening the regularization, increasing the model order (more parameters to learn), etc.

3. If $E_{train}$ is close to zero and $E_{hold-out}$ is not (possibly overfitting), it might be beneficial to decrease the model flexibility by tightening the regularization, decreasing the order (fewer parameters to learn), etc.

(page 66-67)