---
title: "part1"
author: "Cui Qingxuan"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: true
    latex_engine: xelatex
---

# Assignment 1

```{r include=FALSE}
data = read.csv("data/tecator.csv")
head(data)
```

```{r include=FALSE}
dataProcessing = function(data, colnames){
    for(i in colnames)
        data[[i]] = NULL
    return(data)
}

dataSplit = function(data, proportion){
    set.seed(12345)
    n = dim(data)[1]
    id = sample(1:n,floor(n*0.5))
    train_data = data[id,]
    test_data = data[-id,]
    data_list = list()
    data_list$train = train_data
    data_list$test = test_data
    return(data_list)
}

computeMSE = function(model ,data, lambda = NULL){
    if(!is.null(lambda)){
      x = as.matrix(data[, -which(names(data) == "Fat")]) 
      y = data$Fat 
      pred = predict(model, newx = x, s = lambda)
      mse = mean((y - pred) ** 2)
    }
    else{
      pred = predict(model, data)
      mse = mean((data$Fat - pred) ** 2)
      return(mse)
    }
}
```

## Report the underlying probabilistic model and comment on the quality of fit and prediction and model.

```{r include=FALSE}
data = dataProcessing(data, c("Sample", "Protein", "Moisture"))
data_list = dataSplit(data, proportion = 0.5)
train_data = data_list$train
test_data = data_list$test

lr = lm(Fat ~ ., data = train_data)
trainMSE = computeMSE(lr, train_data)
testMSE = computeMSE(lr, test_data)
```

The probability model is:$$
  \text{Y}_i = \beta_0 + \beta_1 \cdot \text{Channel}_{1} + \beta_2 \cdot \text{Channel}_{2} + \cdots + \beta_{100} \cdot \text{Channel}_{100} + \epsilon, \quad \epsilon \sim N(0, \sigma^2)
$$


```{r echo=FALSE}
cat("The MSE of training data is:",trainMSE,"\n")
cat("The MSE of test data is:",testMSE,"\n")
```

The model performs well on the training data (MSE = 0.0057), but has a large error on the test data (MSE = 722.4294), indicating that the model cannot be generalized and there is a serious overfitting phenomenon.

## Report the cost function

```{r include=FALSE}
library(glmnet)
x_train = as.matrix(train_data[, -which(names(train_data) == "Fat")]) 
y_train = train_data$Fat  
lasso_model = glmnet(x_train, y_train, alpha = 1)

# Plot the coefficient paths
png("fig/p1_img1.png", width = 1800, height = 1200, res = 300)
plot(lasso_model, xvar = "lambda", label = TRUE)
axis(1, at = log(lasso_model$lambda), labels = floor(lasso_model$lambda)) 
```

The cost function for **LASSO regression** is defined as:

$J(\beta) = \frac{1}{2n} \sum_{i=1}^n \big(y_i - \mathbf{X}_i \cdot \beta \big)^2 + \lambda \sum_{j=1}^p |\beta_j|$

Where lambda controls the regularization strength.

## interpret LASSO Coefficient Paths Plot

```{r echo=FALSE}

plot(lasso_model, xvar = "lambda", label = TRUE)
title(main = "LASSO Coefficient Paths")
```

1.  This graph shows the path that the regression coefficient of each feature in the **LASSO regression model** changes with the regularization parameter $\log(\lambda)$ : the horizontal axis is $\log(\lambda)$, and the vertical axis is the value of the regression coefficient for each feature.

2.  With the increase of $\lambda$ (from right to left), the regularization force is enhanced, and the coefficients of most features are compressed to 0, leaving only a few important features retaining non-zero coefficients, which realizes feature selection.

## model with only three features

```{r echo=FALSE}
# The value of lambda that results in only three non-zero features.
for (i in seq_along(lasso_model$lambda)) {
  coefs <- coef(lasso_model, s = lasso_model$lambda[i])
  non_zero_count <- sum(coefs != 0) - 1  
  if (non_zero_count == 3) {
    lambda_selected <- lasso_model$lambda[i]
    cat("Lambda with 3 non-zero coefficients:", lambda_selected, "\n")
    break
  }
}

```

```{r echo=FALSE}
coefs <- coef(lasso_model, s = lambda_selected)
coefs_selected_list <- which(coefs!=0)
coefs_selected_list <- coefs_selected_list[-1] #without intercept
coefs_selected <- coefs[coefs_selected_list,]
cat("The remaining features are:", "\n")
print(coefs_selected)

```

## Fit Ridge regression and compare the plots obtained in steps 3 and 4

```{r echo=FALSE}
ridge_model = glmnet(x_train, y_train, alpha = 0)

plot(ridge_model, xvar = "lambda", label = TRUE)
axis(1, at = log(ridge_model$lambda), labels = floor(ridge_model$lambda)) 


```

LASSO coefficient path diagram:

As $\lambda$ increases, the coefficients of many features are compressed to 0, and the model becomes sparse.

Ridge coefficient path map:

As $\lambda$ increases, the coefficients of all features gradually shrink, but they do not become zero.

```{r include=FALSE}
cv_lasso = cv.glmnet(x_train, y_train, alpha = 1)
cv_scores = cv_lasso$cvm
# Load necessary libraries
library(ggplot2)

# Extract data for plotting
plot_data = data.frame(
  log_lambda = log(cv_lasso$lambda),  # Log of lambda values
  cv_scores = cv_lasso$cvm           # CV scores (MSE)
)

# Create ggplot
p3 = ggplot(plot_data, aes(x = log_lambda, y = cv_scores)) +
    geom_line(color = "black") +         # Line connecting the points
    geom_point(size = 2, color = "black") +  # Points for CV scores
    geom_vline(xintercept = log(cv_lasso$lambda.min), color = "red", linetype = "dashed", size = 1) +  # Optimal lambda line
    geom_vline(xintercept = log(cv_lasso$lambda.1se), color = "blue", linetype = "dashed", size = 1) + # 1 SE lambda line
    labs(
      title = "Dependence of CV Score on log(lambda)",
      x = "log(lambda)",
      y = "CV Score (Mean Squared Error)"
    ) +
    annotate("text", x = log(cv_lasso$lambda.min), y = min(cv_lasso$cvm)+10, 
             label = "lambda.min", color = "red", hjust = -0.2, vjust = -0.5) +
    annotate("text", x = log(cv_lasso$lambda.1se), y = min(cv_lasso$cvm)+20, 
             label = "lambda.1se", color = "blue", hjust = -0.2, vjust = -0.5) +
    theme_minimal()
ggsave("fig/p1_pic3.png", plot = p3, width = 6, height = 4, dpi = 300)

optimal_lambda = cv_lasso$lambda.min
cat("the optimal lambda: ", optimal_lambda,"\n")
coef = coef(cv_lasso, s = optimal_lambda)
var_num = sum(coef != 0) - 1
cat("the number of variables: ", var_num,"\n")

mse_optimal_lam = computeMSE(cv_lasso, test_data, lambda = optimal_lambda)
logminus4_lambda = cv_lasso$lambda[which.min(abs(log(cv_lasso$lambda) + 4))]
mse_certain_lam = computeMSE(cv_lasso, test_data, lambda = logminus4_lambda)
cat("lambda = -4, mse = ", mse_certain_lam, "\n")
cat("lambda = optimal lambda, mse = ", mse_optimal_lam, "\n")
```

## Task 5

### Dependence of the CV score on $\log(\lambda)$

```{r echo=FALSE}
p3
```


Dependence of CV score on $\log(\lambda)$:
As $\log(\lambda)$ increases, the CV scores increase.At the optimal $\log(\lambda)$, the cv score is the lowest, indicating the best performance. When the $\log(\lambda)$ value is large, the effect of the regularization term increases, forcing the model coefficients to shrink, leading to underfitting.

### Optimal $lambda$

```{r echo=FALSE}
cat("the optimal lambda: ", optimal_lambda,"\n")
coef = coef(cv_lasso, s = optimal_lambda)
var_num = sum(coef != 0) - 1
cat("the number of variables: ", var_num,"\n")

mse_optimal_lam = computeMSE(cv_lasso, test_data, lambda = optimal_lambda)
logminus4_lambda = cv_lasso$lambda[which.min(abs(log(cv_lasso$lambda) + 4))]
mse_certain_lam = computeMSE(cv_lasso, test_data, lambda = logminus4_lambda)
cat("log_lambda = -4, mse = ", mse_certain_lam, "\n")
cat("lambda = ",optimal_lambda, "mse = ", mse_optimal_lam, "\n")
```

The difference in MSE values is very small, approximately 0.19, which is unlikely to be statistically significant.

### Scatter plot of the original test versus predicted test values

```{r include=FALSE}
y_test = test_data$Fat
x_test = test_data[, -which(names(test_data) == "Fat")]
y_pred = predict(cv_lasso, s = optimal_lambda, newx = as.matrix(x_test))

p4 = ggplot(data = cbind(y_test, y_pred), mapping = aes(x = y_test, y = y_pred)) +
  geom_point(size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", linewidth = 1) +
  labs(title = "Labels in Test vs Predicted Labels",
       x = "Labels in Test",
       y = "Predicted Labels") +
  theme_minimal()

```

```{r echo=FALSE}
p4
```

The model performs well overall, as most predictions are close to the actual values.

The scatterplot shows that the model captures the general trend of the data, with no significant systematic errors.

But a few outliers at higher values suggest the model may struggle with extreme cases or higher variability in predictions for larger labels.


























# Assignment 2

## Report how changing the deviance and node size affected the size of the trees and explain why

```{r, include=FALSE}
library(tree)
library(rpart)
library(pROC)
data <- read.csv("data/bank-full.csv", header = TRUE, sep = ";")
data <- data[,-12]
data$y <- as.factor(data$y)
characer_col <- c()
for (i in 1:15) {
  if(is.character(data[,i])==TRUE){
    characer_col <- c(characer_col,i)
  }
}
for (col in characer_col) {
  data[,col] <- as.factor(data[,col])
}

n <- nrow(data)
set.seed(12345)
id <- sample(1:n,floor(n*0.4))
train <- data[id,]

valid_test <- data[-id,]
id_2 <- sample(1:nrow(valid_test),floor(nrow(valid_test)*0.5))
valid <- valid_test[id_2,]
test <- valid_test[-id_2,]

# default setting
bank_tree <- tree(y ~ .,data = train)

pred_default_train <- predict(bank_tree, train, type = "class")
error_default_train <- mean(pred_default_train != train$y) # Misclassfication rate of train
table(train$y,pred_default_train)

pred_default_valid <- predict(bank_tree, valid, type = "class")
error_default_valid <- mean(pred_default_valid != valid$y) # Misclassfication rate of valid
table(valid$y,pred_default_valid)

# Decision Tree with smallest allowed node size equal to 7000
bank_tree_2 <- tree(y ~ ., data = train, control = tree.control(nobs = nrow(train), minsize = 7000))

pred_2_train <- predict(bank_tree_2, train, type = "class")
error_2_train <- mean(pred_2_train != train$y) # Misclassfication rate of train
table(train$y,pred_2_train)

pred_2_valid <- predict(bank_tree_2, valid, type = "class")
error_2_valid <- mean(pred_2_valid != valid$y) # Misclassfication rate of valid
table(valid$y,pred_2_valid)

# Decision trees minimum deviance to 0.0005
bank_tree_3 <- tree(y ~ ., data = train, control = tree.control(nobs = nrow(train), mindev = 0.0005))

pred_3_train <- predict(bank_tree_3, train, type = "class")
error_3_train <- mean(pred_3_train != train$y) # Misclassfication rate of train
table(train$y,pred_2_train)

pred_3_valid <- predict(bank_tree_3, valid, type = "class")
error_3_valid <- mean(pred_3_valid != valid$y) # Misclassfication rate of valid
table(valid$y,pred_3_valid)
```

## report how changing the deviance and node size affected the size of trees and explain why
```{r}
model_list = list(default = bank_tree, minsize = bank_tree_2, mindiv = bank_tree_3)
depth = vector()
nodes = vector()
for(i in model_list){
  depth = c(depth, max(attr(i$frame, "row.names")))
  nodes = c(nodes, nrow(i$frame))
}
default_parameters = tree.control(nobs = 1)[c("minsize", "mindev")]
report = data.frame(model_name = c("default parameters model", "minsize = 7000 model", "mindiv = 0.0005 model"),
                    mindiv = c(default_parameters$mindev, default_parameters$mindev, 0.0005),
                    minsize = c(default_parameters$minsize, 7000, default_parameters$minsize),
                    depth = depth,
                    nodes = nodes)

library(knitr)
kable(report, format = "markdown") 
```

comment:
In the comparison between the minsize = 7000 model and the default one, obviously, it represents that the model is simplified with minsize increasing, while the size of model significantly increases when mindiv decreases.
explanation:
The minsize of the decision tree indicates the minimum number of samples allowed per leaf node, and a larger minsize, when minsize increases, the leaf nodes need to contain more samples to continue splitting, which limits the depth of the split and makes the model simpler. mindiv is the minimum boost to the target variable splitting criterion when splitting a node (e.g. based on information gain or Gini index). As mindiv decreases, the model allows for finer splitting criteria, which increases the complexity of the model (i.e., the number of tree layers and leaf nodes increases significantly).

```{r}
# task 3
trainScore = rep(0, 50)
testScore = rep(0, 50)

for (i in 2:50) {
  prunedTree <- prune.tree(bank_tree_3, best = i)
  # Predict on training and validation data
  pred_train <- predict(prunedTree, newdata = train,type = "tree")
  pred_valid <- predict(prunedTree, newdata = valid,type = "tree")
  
  # Compute misclassification rates
  #trainScore[i - 1] <- mean(pred_train != train$y)
  #testScore[i - 1] <- mean(pred_valid != valid$y)
  trainScore[i - 1] <- deviance(pred_train)
  testScore[i - 1] <- deviance(pred_valid)

}

plot(2:50, trainScore[2:50], type="b", col="red", 
     ylim=c(7800,11850))
points(2:50, testScore[2:50], type="b", col="blue")
```

## Report the optimal amount of leaves and which variables seem to be most important for decision making in this tree

```{r}
# find optimal size
best_size_index <- which(testScore==min(testScore))
best_size <- best_size_index + 2
optimal_tree <- prune.tree(bank_tree_3, best = best_size)

# summary(optimal_tree)
```

```{r}
split_info = optimal_tree$frame
variable_importance = tapply(split_info$dev, split_info$var, sum, na.rm = TRUE)
variable_importance = variable_importance[names(variable_importance) != "<leaf>"]
variable_importance = sort(variable_importance, decreasing = TRUE)
cat("the most important variable is ", names(variable_importance[which.max(variable_importance)]), "\n")
cat("the optimal amout of leaves: ", best_size, "\n")
```


```{r}
plot(optimal_tree, type = "uniform", margin = 0.1)
text(optimal_tree, use.n = TRUE, cex = 0.5)
```
## Interpret the information provided by the tree structure

From the tree structure we can observe that: 1. "month" is used as a split condition for most times, 2. Most leaves have a classification label of no, 3. Most leaf nodes labeled yes are in the poutcome value of whether or not they fall in the abd.

## Comment whether the model has a good predictive power and which of the measures should be preferred here
```{r}
# task 4
# optimal_pred <- predict(optimal_tree, newdata = test, type = "tree")
optimal_pred <- predict(optimal_tree, newdata = test, type = "class")

# confusion matrix
conf_matrix <- table(Observed = test$y,Predicted = optimal_pred)
print(conf_matrix)

# accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy:", round(accuracy, 3)))

# F1-score
TP <- conf_matrix["yes", "yes"]
FP <- conf_matrix["no", "yes"]
FN <- conf_matrix["yes", "no"]

precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

f1_score <- 2 * (precision * recall) / (precision + recall)
print(paste("F1-score:", round(f1_score, 3)))
```
comment: niu bi!

## Compute the TPR and FPR values for the two models and plot the corresponding ROC curves. Conclusion? Why precision-recall curve could be a better option here?

```{r}
# task 5
loss_matrix <- matrix(c(0, 5, 1, 0), nrow = 2)
colnames(loss_matrix) <- c("no", "yes")
rownames(loss_matrix) <- c("no", "yes")


tree_with_loss <- rpart(y ~ ., data = train, parms = list(loss = loss_matrix))
pred_with_loss <- predict(tree_with_loss, newdata = test, type = "class")


conf_matrix_loss <- table(Observed = test$y,Predicted = pred_with_loss)
print(conf_matrix_loss)

accuracy_loss <- sum(diag(conf_matrix_loss)) / sum(conf_matrix_loss)

#F1-score with loss
TP_l <- conf_matrix_loss["yes", "yes"]
FP_l <- conf_matrix_loss["no", "yes"]
FN_l <- conf_matrix_loss["yes", "no"]

precision_l <- TP_l / (TP_l + FP_l)
recall_l <- TP_l / (TP_l + FN_l)
f1_score_loss <- 2 * (precision_l * recall_l) / (precision_l + recall_l)

print(paste("Accuracy with loss matrix:", round(accuracy_loss, 3)))
print(paste("F1-Score with loss matrix:", round(f1_score_loss, 3)))

```


```{r}
# table
performance = data.frame(accuracy = c(round(accuracy, 3), round(accuracy_loss, 3)), f1_score = c(round(f1_score, 3), round(f1_score_loss, 3)))
row.names(performance) = c("optimal tree", "tree with loss matrix")
colnames(performance) = c("accuracy", "F1-Score")
kable(performance, format = "markdown")
```

comment: 
bulabulabula

## task 6

```{r}
# task 6

# set models
tree_pred_prob <- predict(optimal_tree, newdata = test, type = "vector")
tree_probs_yes <- tree_pred_prob[, "yes"]

log_model <- glm(y ~ ., data = train, family = "binomial")
log_pred_prob <- predict(log_model, newdata = test, type = "response")

thresholds <- seq(0.05, 0.95, by = 0.05)

tree_tpr <- tree_fpr <- log_tpr <- log_fpr <-precision_tree <- recall_tree <-  precision_log <-recall_log <-   numeric(length(thresholds)) # initiate value

for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  
  tree_pred <- ifelse(tree_probs_yes > threshold, "yes", "no")
  tree_cm <- table(Observed = test$y, Predicted = factor(tree_pred, levels = c("no", "yes")))
  
  # compute TPR and FPR 
  tree_tpr[i] <- tree_cm["yes", "yes"] / sum(tree_cm["yes", ])
  tree_fpr[i] <- tree_cm["no", "yes"] / sum(tree_cm["no", ])
  
  # compute precision and recall
  precision_tree[i] <- tree_cm["yes", "yes"] / (tree_cm["yes", "yes"] + tree_cm["no", "yes"])
  recall_tree[i] <- tree_cm["yes", "yes"] / (tree_cm["yes", "yes"] + tree_cm["yes", "no"])
  
  # result of logistic
  log_pred <- ifelse(log_pred_prob > threshold, "yes", "no")
  log_cm <- table(Observed = test$y,Predicted = factor(log_pred, levels = c("no", "yes")))
  
  # compute TPR and FPR (logistic)
  log_tpr[i] <- log_cm["yes", "yes"] / sum(log_cm["yes", ])
  log_fpr[i] <- log_cm["no", "yes"] / sum(log_cm["no",])
  
  # compute precision and recall (logistic)
  precision_log[i] <- log_cm["yes", "yes"] / (log_cm["yes", "yes"] + log_cm["no", "yes"])
  recall_log[i] <- log_cm["yes", "yes"] / (log_cm["yes", "yes"] + log_cm["yes", "no"])
}

# Plot ROC curve for the Optimal Decision Tree
plot(tree_fpr, tree_tpr, type = "b", col = "blue", pch = 16,
     xlab = "False Positive Rate (FPR)", ylab = "True Positive Rate (TPR)",
     main = "ROC Curves for Decision Tree and Logistic Regression")

# Add ROC curve for Logistic Regression
lines(log_fpr, log_tpr, type = "b", col = "red", pch = 17)

# Add legend
legend("bottomright", legend = c("Decision Tree", "Logistic Regression"),
       col = c("blue", "red"), pch = c(16, 17))


# precision-recall curve
plot(recall_tree, precision_tree, type = "b", col = "blue", pch = 16,
     xlab = "Recall", ylab = "Precision",
     main = "Precision-Recall Curves")

# Add ROC curve for Logistic Regression
lines(recall_log, precision_log, type = "b", col = "red", pch = 17)

# Add legend
legend("bottomright", legend = c("Decision Tree", "Logistic Regression"),
       col = c("blue", "red"), pch = c(16, 17))

```
Comment: bulabulabula



# Assignment 3

```{r include=FALSE}
communities <- read.csv("data/communities.csv")

#task 1
#scale data except of `ViolentCrimesPerPop`
scaled <- scale(communities[,-which(names(communities)=="ViolentCrimesPerPop")],center=TRUE,scale=TRUE)

#compute eigenvectors and eigenvalues
covariance<- cov(scaled)

#extract eigenvectors and eigenvalues
eigen <- eigen(covariance)
eigen_values <- eigen$values
eigen_vectors <- eigen$vectors
eigen_values
eigen_vectors

#compute total variance
variance <- sum(eigen_values)

#compute contribution of each component
propotion_variance <- eigen_values/variance
propotion_variance

#compute cumulative variance proportion
cumulative_var <- cumsum(propotion_variance)
cumulative_var

#numbers of components to obtain 95% variance
index <-min(which(cumulative_var>0.95))
cat("At least ",index,"components to obtain a 95% of variance in the data.")

#the proportion of variance explained by each of the first two principal component.
first_com_proportion <- propotion_variance[1]
second_com_proportion <- propotion_variance[2]
cat("The proportion of variance of explained by each of the two principal components:\n")
cat("The first components: ",first_com_proportion ,"\n")
cat("The second components: ",second_com_proportion,"\n")




#task 2
library(ggplot2)

#using princomp
prin <- princomp(scaled)

#extract the first principle component
first_com_prin <- prin$scores[,1]

#make the trace plot of the first principle component
df_first <- data.frame(index=1:length(first_com_prin),
                       first_principle_component=first_com_prin)
ggplot(df_first,aes(x=index,y=first_principle_component))+
        geom_line()+
        labs(title = "The trace of the first principle component",x="Index",y="The First Principle Component")+
        theme_minimal()

#Do many features have a notable contribution to this component

#Top 5 features contribute mostly
first_com_contribute <- prin$loadings[,1]
first_com_contribute_df <- data.frame(
  Feature=names(first_com_contribute),
  contribution=first_com_contribute,
  contribution_Abs=abs(first_com_contribute)
)
#extract top 5 features 
top_5_features <- first_com_contribute_df$Feature[order(first_com_contribute_df$contribution_Abs,decreasing = TRUE)[1:5]]
cat("Top 5 features that contributed mostly to the first principle component: ",top_5_features,".\n")


#Relationship to crime level

#medFamInc:median family income (differs from household income for non-family households
#medIncome:median household income 
#PctKids2Par: percentage of kids in family housing with two parents
#pctWInvInc:percentage of households with investment / rent income in 1989
#PctPopUnderPov:percentage of people under the poverty level

# Conclusion: These five features mainly describes the following aspects: 1.Median family income;2.Median household income;3.Percentage of kids in family housing with two parents;4.Percentage of households with investment / rent income;5.Percentage of people under the poverty level.
#All five features are related to family property and household wealth levels.The logical relationship to  crime levels is that  families with fewer financial resources or  more children(which increases financial budget)may be more likely to resort to illegal activities to obtain money.Financial stress within household may be a significant factor contributing to crime level. 



#Plot the pc scores in the coordinates(PC1,PC2)
second_com_prin <- prin$scores[,2]
fir_sec_prin_df <- data.frame(
  PC1=first_com_prin,
  PC2=second_com_prin,
  ViolentCrimesPerPop=communities$ViolentCrimesPerPop
)
ggplot(fir_sec_prin_df,aes(x=PC1,y=PC2,color=ViolentCrimesPerPop))+
  geom_point()+
  scale_color_viridis_c()+
  labs(tittle="PC scores in the coordinates(PC1,PC2)",X="PC1",y="PC2",color="ViolentCrimesPerPop")+
  theme_minimal()
  
#task3
#split the data(50/50)
library(caret)
set.seed(12345)
n <- nrow(communities)
id <- sample(1:n,floor(n*0.5))
train <- communities[id,]
test <- communities[-id,]

scaler <- preProcess(train)
trainS <- predict(scaler,train)
testS <- predict(scaler,test)

#fit the model using scaled training data
fit <- lm(ViolentCrimesPerPop~.-1,data=trainS)

predictions_train <- predict(fit,newdata=trainS)
predictions_test <- predict(fit,newdata=testS)

#MSE for training and test data
MSE_training <- mean((trainS$ViolentCrimesPerPop-predictions_train)^2)
MSE_test <- mean((testS$ViolentCrimesPerPop-predictions_test)^2)
cat("MSE for the training data is :",MSE_training,"\n")
cat("MSE for the test data is :",MSE_test,"\n")

#Comment on the quality of model
R2 <- summary(fit)$r.squared
cat("The R^2 value for the model is: ",R2,"\n")

#Comment:The Mean Squared Error (MSE) for the training data is 0.2752071, indicating that the model performs well on the training set. However, the MSE for the test data is 0.4248011, which is higher than the training data MSE. This suggests that the model may be overfitting, as it performs worse and may not generalize as well on unseen data . The R² value is 0.7245166, meaning that the model explains approximately 72.4% of the variance in the target variable. 

#task4
#implement cost function without intercept
costfunction <- function(theta,training_X,training_y,test_X,test_y){
  #compute the mean squared error
  train_error<- mean((training_X%*%theta-training_y)^2)
  test_error <- mean((test_X%*%theta-test_y)^2)
  return(c(train_error,test_error))
}

training_X <- as.matrix(trainS[,-101])
training_y <- trainS$ViolentCrimesPerPop

test_X <- as.matrix(testS[,-101])
test_y <- testS$ViolentCrimesPerPop

#optimize theta and compute mse
theta_initial <- rep(0,ncol(training_X))


#create a df to store errors each iteration
errors_iteration_df <- data.frame(
  iteration=integer(),
  train_errors=numeric(),
  test_errors=numeric()
)

optim_theta <- optim(par=theta_initial,
                     fn=function(theta){
                       #compute training and test errors for each iteration
                       errors <- costfunction(theta,training_X,training_y,test_X,test_y)
                       
                       #store the errors 
                       errors_iteration_df <<- rbind(errors_iteration_df,data.frame(
                         iteration=nrow(errors_iteration_df)+1,
                         train_errors=errors[1],
                         test_errors=errors[2]
                       ))
                       
                       #return training errors since `optim()` uses training error 
                       return(errors[1])
                       
                     },
                    
                     method = "BFGS")
#print(errors_itteration_df)

#plot showing the dependence if both errors on the iteration number

ggplot(errors_iteration_df[errors_iteration_df$iteration >500,],aes(x=iteration))+   #discard the first 500 iterations to make it visible
  geom_line(aes(y=train_errors,color="Training Errors"))+
  geom_line(aes(y=test_errors,color="Test Errors"))+
  labs(title = "Training and test error during aech iteration",x="Iteration number",y="Errors")+
  scale_color_manual(values = c("blue","red"))+
  scale_y_continuous(limits = c(0, 1.2), breaks = seq(0, 1.2, by = 0.2)) +
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5))

print(errors_iteration_df$train_errors)

#find optimal iteration
index <- which.min(errors_iteration_df[,3])
cat("The optimal iteration number is:",index,"\n")

#extract training and test error at optimal iteration
optimail_train_errors <- errors_iteration_df[index,2]
optimai_test_errors <- errors_iteration_df[index,3]


cat("Training error at optimal iteration",optimail_train_errors,"\n")
cat("Test error at optimal iteration",optimai_test_errors,"\n")

```

## Task 1

```{r echo=FALSE}
cat("At least ",index,"components to obtain a 95% of variance in the data.")
```

```{r echo=FALSE}
cat("The proportion of variance of explained by each of the two principal components:\n")
cat("The first components: ",first_com_proportion ,"\n")
cat("The second components: ",second_com_proportion,"\n")
```

## Task 2

### Trace plot of the first principle component.

### Top 5 contributing features

```{r echo=FALSE}
cat("Top 5 features that contributed mostly to the first principle component: ",top_5_features,".\n")
```

medFamInc:median family income (differs from household income for non-family households

medIncome:median household income

PctKids2Par: percentage of kids in family housing with two parents

pctWInvInc:percentage of households with investment / rent income in 1989

PctPopUnderPov:percentage of people under the poverty level

Conclusion:

These five features mainly describes the following aspects:

1.Median family income; 2.Median household income; 3.Percentage of kids in family housing with two parents; 4.Percentage of households with investment / rent income; 5.Percentage of people under the poverty level.

All five features are related to family property and household wealth levels. The logical relationship to crime levels is that families with fewer financial resources or more children (which increases financial budget) may be more likely to resort to illegal activities to obtain money. Financial stress within household may be a significant factor contributing to crime level.

### plot of the PC scores in the coordinates (PC1, PC2)

```{r echo=FALSE}
ggplot(fir_sec_prin_df,aes(x=PC1,y=PC2,color=ViolentCrimesPerPop))+
  geom_point()+
  scale_color_viridis_c()+
  labs(tittle="PC scores in the coordinates(PC1,PC2)",X="PC1",y="PC2",color="ViolentCrimesPerPop")+
  theme_minimal()
```

1.  The score of the data point on the first principal component (PC1) is significantly positively correlated with the violent crime rate ("ViolentCrimesPerPop"), which tends to increase when PC1 increases (yellow).

2.  The second principal component (PC2) has a weak ability to distinguish the violent crime rate, and the distribution of data points in this direction is relatively symmetrical, and there is no obvious pattern.

3.  The overall arc distribution of the data indicates that there may be a nonlinear relationship, and further analysis of the characteristic contribution of PC1 is needed to understand the key factors affecting the violent crime rate.

## Task 3

After training the linear regression model.

```{r echo=FALSE}
cat("MSE for the training data is :",MSE_training,"\n")
cat("MSE for the test data is :",MSE_test,"\n")
cat("The R^2 value for the model is: ",R2,"\n")
```

1.  The MSE of the model on the training data is 0.275, and the MSE on the test data is 0.425. The test error is slightly higher but generally close, indicating that the model is stable and the degree of overfitting is low.

2.  The coefficient of determination R² is 0.725, indicating that the model can explain 72.5% of the variance of violent crime rate, but 27.5% of the variation is still unexplained.

3.  A slight increase in the test error may indicate the need to optimize features or introduce regularization methods to further improve the generalization ability of the model.

## Task 4

# Assignment 4. Theory

## What are the practical approaches for reducing the expected new data error, according to the book?

To achieve minimal $E_{new}$, according to the decomposition $E_{new}$ = $E_{train}$ $+$ generalization gap, we need to have $E_{train}$ as well as the generalization gap small.

1. Increasing the size of the training data to reduce the generalization gap and $E_{new}$;

2. If $E_{hold-out}$ $\approx$ $E_{train}$ (small generalization gap; possibly underfitting), it might be beneficial to increase the model flexibility by loosening the regularization, increasing the model order (more parameters to learn), etc.

3. If $E_{train}$ is close to zero and $E_{hold-out}$ is not (possibly overfitting), it might be beneficial to decrease the model flexibility by tightening the regularization, decreasing the order (fewer parameters to learn), etc.

(page 66-67)

## What important aspect should be considered when selecting minibatches, according to the book? 

1.It's important to ensure the different mini-batches are balanced and representative for the whole dataset.If we have a few different output classes and the dataset is sorted with respect to the output, the mini-batch with the first n data points would only include one class and not give a good approximation of the gradient for the full dataset.

2.The mini-batches should be formed randomly. To ensure each mini-batch with representativeness, the training data should be shuffled randomly before diving into mini-batches. And after completing one epoch, the dataset should be reshuffled before another epoch.

(page 125)



