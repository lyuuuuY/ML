---
title: "report_lab1_block2"
author: " Yanjie Lyu, Yi Yang,Qingxuan Cui"
date: "2024-11-26"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Contributions
 The contributions are distributed as follows:  
 Qingxuan Cui:    
 Yanjie Lyu:  
 Yi Yang: Worked on assignment 2 and question 2 from assignment 4.After completing their respective assignments(including code writing and analysis), all results were shared and thoroughly discussed among the three members.<br>
 

```{r , include=FALSE}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log lik between two consecutive iterations
n=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=n, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3,1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)

plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")

# Producing the training data
for(i in 1:n) {
  m <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[i,d] <- rbinom(1,1,true_mu[m,d])
  }
}

M=2 # number of clusters
w <- matrix(nrow=n, ncol=M) # weights
pi <- vector(length = M) # mixing coefficients
mu <- matrix(nrow=M, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations

# Random initialization of the parameters
pi <- runif(M,0.49,0.51)
pi <- pi / sum(pi)
for(m in 1:M) {
  mu[m,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1))
  points(mu[2,], type="o", col="red")
  #points(mu[3,], type="o", col="green")
  #points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.5)
  # E-step: Computation of the weights
  
  #Bern <- vector(length = n)
  #epsilon <- 1e-10
  #mu <- pmax(mu, epsilon)
  #mu <- pmin(mu, 1 - epsilon)
  
  p_x <- numeric(length = n)
  for (i in 1:n) {
    pi_bern <- numeric(M)  
    Bern <- rep(1, M) 
    max_Bern <- 0
    for (m in 1:M){
      Bern[m] <- 1
      for (j in 1:D) {
        if(x[i,j]==1){
          Bern[m] <- Bern[m]*mu[m,j]
        }else{Bern[m] <- Bern[m]*(1-mu[m,j])}
      }
      pi_bern[m] <- pi[m]*Bern[m]
      p_x[i] <- p_x[i] + pi_bern[m]
    }
    for (m in 1:M) {
      w[i,m] <- pi_bern[m]/p_x[i]
    }
    
  }
  
  
  
  #Log likelihood computation.
  
  log_p_x <- log(p_x)
  llik[it] <- sum(log_p_x)
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the lok likelihood has not changed significantly
  
  if(it > 1 && abs(llik[it] - llik[it - 1]) <= min_change){
    print(it)
    break
  }
  
  #M-step: ML parameter estimation from the data and weights
  for (m in 1:M) {
    N_m <- sum(w[,m])
    pi[m] <- N_m / n
    for (d in 1:D) {
      mu[m,d] <- sum(w[,m] * x[,d]) / N_m
    }
    
  }
}


pi
mu
plot(llik[1:it], type="o")
```

# Assignment 2

## Set M=2,3,4,the results show as follows


Set M=2,the results show as follows:
```{r echo=FALSE}
print(it)
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
plot(llik[1:it], type="o")
pi
mu
```



```{r , include=FALSE}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log lik between two consecutive iterations
n=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=n, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")

# Producing the training data
for(i in 1:n) {
  m <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[i,d] <- rbinom(1,1,true_mu[m,d])
  }
}

M=3 # number of clusters
w <- matrix(nrow=n, ncol=M) # weights
pi <- vector(length = M) # mixing coefficients
mu <- matrix(nrow=M, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations

# Random initialization of the parameters
pi <- runif(M,0.49,0.51)
pi <- pi / sum(pi)
for(m in 1:M) {
  mu[m,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1))
  points(mu[2,], type="o", col="red")
  points(mu[3,], type="o", col="green")
  #points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.5)
  # E-step: Computation of the weights
  
  #Bern <- vector(length = n)
  #epsilon <- 1e-10
  #mu <- pmax(mu, epsilon)
  #mu <- pmin(mu, 1 - epsilon)
  
  p_x <- numeric(length = n)
  for (i in 1:n) {
    pi_bern <- numeric(M)  
    Bern <- rep(1, M) 
    max_Bern <- 0
    for (m in 1:M){
      Bern[m] <- 1
      for (j in 1:D) {
        if(x[i,j]==1){
          Bern[m] <- Bern[m]*mu[m,j]
        }else{Bern[m] <- Bern[m]*(1-mu[m,j])}
      }
      pi_bern[m] <- pi[m]*Bern[m]
      p_x[i] <- p_x[i] + pi_bern[m]
    }
    for (m in 1:M) {
      w[i,m] <- pi_bern[m]/p_x[i]
    }
    
  }
  
  
  
  #Log likelihood computation.
  
  log_p_x <- log(p_x)
  llik[it] <- sum(log_p_x)
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the lok likelihood has not changed significantly
  
  if(it > 1 && abs(llik[it] - llik[it - 1]) <= min_change){
    print(it)
    break
  }
  
  
  
  #M-step: ML parameter estimation from the data and weights
  for (m in 1:M) {
    N_m <- sum(w[,m])
    pi[m] <- N_m / n
    for (d in 1:D) {
      mu[m,d] <- sum(w[,m] * x[,d]) / N_m
    }
    
  }
}


pi
mu
plot(llik[1:it], type="o")
```

Set M=3, the results show as follows:
```{r echo=FALSE}

print(it)
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
plot(llik[1:it], type="o")
pi
mu
```

```{r , include=FALSE}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log lik between two consecutive iterations
n=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=n, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")

# Producing the training data
for(i in 1:n) {
  m <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[i,d] <- rbinom(1,1,true_mu[m,d])
  }
}

M=4 # number of clusters
w <- matrix(nrow=n, ncol=M) # weights
pi <- vector(length = M) # mixing coefficients
mu <- matrix(nrow=M, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations

# Random initialization of the parameters
pi <- runif(M,0.49,0.51)
pi <- pi / sum(pi)
for(m in 1:M) {
  mu[m,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1))
  points(mu[2,], type="o", col="red")
  points(mu[3,], type="o", col="green")
  points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.1)
  # E-step: Computation of the weights
  
  #Bern <- vector(length = n)
  #epsilon <- 1e-10
  #mu <- pmax(mu, epsilon)
  #mu <- pmin(mu, 1 - epsilon)
  
  p_x <- numeric(length = n)
  for (i in 1:n) {
    pi_bern <- numeric(M)  
    Bern <- rep(1, M) 
    max_Bern <- 0
    for (m in 1:M){
      Bern[m] <- 1
      for (j in 1:D) {
        if(x[i,j]==1){
          Bern[m] <- Bern[m]*mu[m,j]
        }else{Bern[m] <- Bern[m]*(1-mu[m,j])}
      }
      pi_bern[m] <- pi[m]*Bern[m]
      p_x[i] <- p_x[i] + pi_bern[m]
    }
    for (m in 1:M) {
      w[i,m] <- pi_bern[m]/p_x[i]
    }
    
  }
  
  
  
  #Log likelihood computation.
  
  log_p_x <- log(p_x)
  llik[it] <- sum(log_p_x)
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the lok likelihood has not changed significantly
  
  if(it > 1 && abs(llik[it] - llik[it - 1]) <= min_change){
    print(it)
    break
  }
  
  
  
  #M-step: ML parameter estimation from the data and weights
  for (m in 1:M) {
    N_m <- sum(w[,m])
    pi[m] <- N_m / n
    for (d in 1:D) {
      mu[m,d] <- sum(w[,m] * x[,d]) / N_m
    }
    
  }
}


pi
mu
plot(llik[1:it], type="o")
```

Set M=4, the results show as follows:
```{r echo=FALSE}
print(it)
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
plot(llik[1:it], type="o")
pi
mu
```

## Compare results

Case of M = 2 :
 The log-likelihood value(-6362.897) is lower than M=3 and M=4, which shows that the model is relatively simple to capture the features of the data. The number of iterations(12) shows that  it converges quickly but to a poor local optimal.

Case of M = 3 :
The log-likelihood value(-6344.5) shows a significant improvement compared to M =2, which indicates the model fits the true distribution of the data more accurately. The number of iterations(26) shows a better balance between model complexity and model performance.

Case of M = 4 :
The log-likelihood value(-6338.288) improves slightly compared to M =3,which means it does not show a significant improvement. The number of iterations(44) indicates increased model complexity complex to M=3, which may result in  overfitting and the model may capture noise in the data.

Conclusion:
A mixture model has too few clusters (e.g.,M=2 in this case) may lead to underfitting and can not capture necessary features from the data.On the contrary, a mixture with too many clusters(e.g.,M=4 in this case) may lead to excessive model complexity with little model performance. And clustering is not necessarily the case to minimize the " clustering loss", so we may perfer a smaller model(e.g.,M=3 in this case) over a large one even if the latter shows a slightly better log-likelihood.







# Assignment 3 Theory

## Impact of Ensemble Size (B) on Model Flexibility




## The loss function used to train the boosted classifier at each iteration.


$$
L(y \cdot f(x)) = \exp(-y \cdot f(x))
$$
where $$ y\cdot f(x)$$ is the margin of the classifier. The ensemble members are added one at a time, and when member b is added, this is done to minimise the exponential loss of the entire ensemble. The reason why we choose the exponential loss is because it results in convenient closed form expressions.
(page 177)

## Use cross-validation to select the number of clusters


# Appendix

##code for assignment 2
```{r , echo=TRUE,eval=FALSE}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log lik between two consecutive iterations
n=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=n, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")

# Producing the training data
for(i in 1:n) {
  m <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[i,d] <- rbinom(1,1,true_mu[m,d])
  }
}

M=3 # number of clusters
#M=2
#M=4
w <- matrix(nrow=n, ncol=M) # weights
pi <- vector(length = M) # mixing coefficients
mu <- matrix(nrow=M, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations

# Random initialization of the parameters
pi <- runif(M,0.49,0.51)
pi <- pi / sum(pi)
for(m in 1:M) {
  mu[m,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
  plot(mu[1,], type="o", col="blue", ylim=c(0,1))
  points(mu[2,], type="o", col="red")
  points(mu[3,], type="o", col="green")
  #points(mu[4,], type="o", col="yellow")
  Sys.sleep(0.5)
  # E-step: Computation of the weights
  
  #Bern <- vector(length = n)
  #epsilon <- 1e-10
  #mu <- pmax(mu, epsilon)
  #mu <- pmin(mu, 1 - epsilon)
  
  p_x <- numeric(length = n)
  for (i in 1:n) {
    pi_bern <- numeric(M)  
    Bern <- rep(1, M) 
    max_Bern <- 0
    for (m in 1:M){
      Bern[m] <- 1
      for (j in 1:D) {
        if(x[i,j]==1){
          Bern[m] <- Bern[m]*mu[m,j]
        }else{Bern[m] <- Bern[m]*(1-mu[m,j])}
      }
      pi_bern[m] <- pi[m]*Bern[m]
      p_x[i] <- p_x[i] + pi_bern[m]
    }
    for (m in 1:M) {
      w[i,m] <- pi_bern[m]/p_x[i]
    }
    
  }
  
  
  
  #Log likelihood computation.
  
  log_p_x <- log(p_x)
  llik[it] <- sum(log_p_x)
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the lok likelihood has not changed significantly
  
  if(it > 1 && abs(llik[it] - llik[it - 1]) <= min_change){
    print(it)
    break
  }
  
  
  
  #M-step: ML parameter estimation from the data and weights
  for (m in 1:M) {
    N_m <- sum(w[,m])
    pi[m] <- N_m / n
    for (d in 1:D) {
      mu[m,d] <- sum(w[,m] * x[,d]) / N_m
    }
    
  }
}


pi
mu
plot(llik[1:it], type="o")
```